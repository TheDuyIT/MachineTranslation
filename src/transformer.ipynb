{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "chinhsua.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheDuyIT/MachineTranslation/blob/master/src/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFG0NDRu5mYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac42ad3-a4ab-4162-957a-fbd2183c8d29"
      },
      "source": [
        "!pip install -q tfds-nightly\r\n",
        "# !pip install tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8MB 20.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEATbN5DAYDT",
        "outputId": "172b3042-55da-4578-8d19-d167baf2f47b"
      },
      "source": [
        "!npx degit TheDuyIT/MachineTranslation -f"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 1 in 1.903s\n",
            "\u001b[36m> destination directory is not empty. Using --force, continuing\u001b[39m\n",
            "\u001b[36m> cloned \u001b[1mTheDuyIT/MachineTranslation\u001b[22m#\u001b[1mmaster\u001b[22m\u001b[39m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB9BXDZLyrMG",
        "outputId": "68623013-76b5-469b-ad68-a7f65ab1882d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjJJyJTZYebt"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from src.process_data import DataFormatter, DataLoader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd1NWMxjfsDd"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2MJM8-jV6IA"
      },
      "source": [
        "loader = DataLoader()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpF0MBnRaq2o"
      },
      "source": [
        "content_en = loader.np_load('lst_cn_1000_original')\n",
        "content_vn = loader.np_load('lst_vi_1000_original')\n",
        "for i in range(len(content_vn)):\n",
        "  content_vn[i] = content_vn[i].lower()\n",
        "  # content_en.append(i['en'].lower())\n",
        "  # content_vn.append(i['vn'].lower())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP0PoNkyDIHp",
        "outputId": "b8a75dad-5d45-4d34-e570-7a8ce08def26"
      },
      "source": [
        "print(len(content_en))\r\n",
        "print(len(content_vn))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4187\n",
            "4187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1guLOWvfLoC"
      },
      "source": [
        "a = tf.data.Dataset.from_tensor_slices(content_en)  # ==> [ 1, 2, 3 ]\n",
        "b = tf.data.Dataset.from_tensor_slices(content_vn)  # ==> [ 4, 5, 6 ]\n",
        "\n",
        "full_dataset = tf.data.Dataset.zip((a, b))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTGUJVei9wjc"
      },
      "source": [
        "train_size = int(0.8*0.9*len(full_dataset)) + 2\n",
        "val_size = int(0.8*0.1*len(full_dataset))\n",
        "test_size = int(0.2*len(full_dataset))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Z3HImR9a-M"
      },
      "source": [
        "full_dataset = full_dataset.shuffle(buffer_size = 1000)\n",
        "train_examples = full_dataset.take(train_size)\n",
        "test_dataset = full_dataset.skip(train_size)\n",
        "val_dataset = test_dataset.skip(val_size)\n",
        "test_dataset = test_dataset.take(test_size)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVBg5Q8tBk5z"
      },
      "source": [
        "# tokenizer_vn = tfds.deprecated.text.SubwordTextEncoder.load_from_file('tokenizer_en')\n",
        "# tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file('tokenizer_vn')\n",
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "tokenizer_vn = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9ddMZP5z3tp"
      },
      "source": [
        "# tokenizer_en.save_to_file('/content/drive/MyDrive/VinBigData/checkpoints/tokenizer_en')\r\n",
        "# tokenizer_vn.save_to_file('/content/drive/MyDrive/VinBigData/checkpoints/tokenizer_vn')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcRp7VcQ5m6g"
      },
      "source": [
        "BUFFER_SIZE = 2000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZwnPr4R055s"
      },
      "source": [
        "def encode(lang1, lang2):\n",
        "  lang1 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "      lang1.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "\n",
        "  lang2 = [tokenizer_vn.vocab_size] + tokenizer_vn.encode(\n",
        "      lang2.numpy()) + [tokenizer_vn.vocab_size+1]\n",
        "  \n",
        "  return lang1, lang2"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mah1cS-P70Iz"
      },
      "source": [
        "def tf_encode(en, vn):\n",
        "  result_en, result_vn = tf.py_function(encode, [en, vn], [tf.int64, tf.int64])\n",
        "  result_en.set_shape([None])\n",
        "  result_vn.set_shape([None])\n",
        "\n",
        "  return result_en, result_vn"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QEgbjntk6Yf"
      },
      "source": [
        "MAX_LENGTH = 190\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c081xPGv1CPI"
      },
      "source": [
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIPFkLr-Qsz"
      },
      "source": [
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "val_dataset = val_dataset.map(tf_encode)\n",
        "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)\n",
        "\n",
        "test_dataset = test_dataset.map(tf_encode)\n",
        "test_dataset = test_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBQuibYA4n0n"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
        "\n",
        "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhIOZjMNKujn"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rz82wEs5biZ"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kLCla68EloE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "0802e8c7-76d3-40e4-e1e2-899040fc87b2"
      },
      "source": [
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 50, 512)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_b4ou4TYqUN"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2i8-e1s8ti9"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVxS8OPI9uI0"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xluDl5cXYy4y"
      },
      "source": [
        "## Scaled dot product attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LazzUq3bJ5SH"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmzGPEy64qmA"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSV3PPKsYecw"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdDqGayx67vv"
      },
      "source": [
        "## Point wise feed forward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBqzJXGfHK3X"
      },
      "source": [
        "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET7xLt0yCT6Z"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7hKcxn6-zd"
      },
      "source": [
        "## Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFv-FNYUmvpn"
      },
      "source": [
        "### Encoder layer\n",
        "\n",
        "Each encoder layer consists of sublayers:\n",
        "\n",
        "1.   Multi-head attention (with padding mask) \n",
        "2.    Point wise feed forward networks. \n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
        "\n",
        "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncyS-Ms3i2x_"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LO_48Owmx_o"
      },
      "source": [
        "### Decoder layer\n",
        "\n",
        "Each decoder layer consists of sublayers:\n",
        "\n",
        "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
        "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
        "3.   Point wise feed forward networks\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
        "\n",
        "There are N decoder layers in the transformer.\n",
        "\n",
        "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SoX0-vd1hue"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE1H51Ajm0q1"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "The `Encoder` consists of:\n",
        "1.   Input Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N encoder layers\n",
        "\n",
        "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpEox7gJ8FCI"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-uO6ls8m2O5"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtT7PKzrXkNr"
      },
      "source": [
        " The `Decoder` consists of:\n",
        "1.   Output Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N decoder layers\n",
        "\n",
        "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5_d5-PLQXwY"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y54xnJnuYgJ7"
      },
      "source": [
        "## Create the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PED3bIpOYkBu"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    \n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "    \n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    \n",
        "    return final_output, attention_weights"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnJn5SLA2ahP"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = tokenizer_en.vocab_size + 2\n",
        "target_vocab_size = tokenizer_vn.vocab_size + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYEGhEOtzn5W"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOmWW--yP3zx"
      },
      "source": [
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYQdOO1axwEI"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r4scdulztRx"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgkDE7hzo8r5"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxGJtoDuYIHL"
      },
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67oqVHiT0Eiu"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phlyxMnm-Tpx"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHumfr7zmMa"
      },
      "source": [
        "## Training and checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiysUa--4tOU"
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOJUSB1T8GjM"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzuf06YZp66w"
      },
      "source": [
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNhuYfllndLZ"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/VinBigData/checkpoints/train1\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKpoA6q1sJFj"
      },
      "source": [
        "EPOCHS = 0"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWZIMEVZ8eTl"
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths o4/1AY0e-g5NwERYfvXHd5gYJ0PO1FU94gGYrmyMq14BU0Dlvo5oiSqu1suLElor variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "# @tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  # print('inner train')\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    # print('---------------------------------')\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H315b1hp8e9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb762245-aa01-48bc-9a0d-ac7a8207d40d"
      },
      "source": [
        "EPOCHS = 2000\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  # inp -> en, tar -> vn\n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "    train_step(inp, tar)\n",
        "    \n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "    \n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 9.1431 Accuracy 0.0000\n",
            "Epoch 1 Loss 9.0765 Accuracy 0.0101\n",
            "Time taken for 1 epoch: 18.40452814102173 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 8.9514 Accuracy 0.1180\n",
            "Epoch 2 Loss 8.7923 Accuracy 0.1426\n",
            "Time taken for 1 epoch: 13.794159889221191 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 8.6332 Accuracy 0.1461\n",
            "Epoch 3 Loss 8.4711 Accuracy 0.1444\n",
            "Time taken for 1 epoch: 13.949857234954834 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 8.2804 Accuracy 0.1430\n",
            "Epoch 4 Loss 8.0625 Accuracy 0.1445\n",
            "Time taken for 1 epoch: 13.927753925323486 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 7.8045 Accuracy 0.1405\n",
            "Saving checkpoint for epoch 5 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-1\n",
            "Epoch 5 Loss 7.5085 Accuracy 0.1444\n",
            "Time taken for 1 epoch: 14.153724670410156 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 7.1795 Accuracy 0.1388\n",
            "Epoch 6 Loss 6.8058 Accuracy 0.1445\n",
            "Time taken for 1 epoch: 14.006246566772461 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 6.3961 Accuracy 0.1442\n",
            "Epoch 7 Loss 5.9855 Accuracy 0.1514\n",
            "Time taken for 1 epoch: 13.894220352172852 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 5.5446 Accuracy 0.1630\n",
            "Epoch 8 Loss 5.1549 Accuracy 0.1650\n",
            "Time taken for 1 epoch: 13.97383427619934 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 4.7468 Accuracy 0.1756\n",
            "Epoch 9 Loss 4.4230 Accuracy 0.1777\n",
            "Time taken for 1 epoch: 14.019425630569458 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 4.1104 Accuracy 0.1904\n",
            "Saving checkpoint for epoch 10 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-2\n",
            "Epoch 10 Loss 3.8664 Accuracy 0.1975\n",
            "Time taken for 1 epoch: 14.362096071243286 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 3.6460 Accuracy 0.2148\n",
            "Epoch 11 Loss 3.4409 Accuracy 0.2624\n",
            "Time taken for 1 epoch: 13.906708240509033 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 3.2308 Accuracy 0.3069\n",
            "Epoch 12 Loss 3.0607 Accuracy 0.3367\n",
            "Time taken for 1 epoch: 13.881892204284668 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.8715 Accuracy 0.3698\n",
            "Epoch 13 Loss 2.6974 Accuracy 0.3797\n",
            "Time taken for 1 epoch: 13.886498212814331 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.5198 Accuracy 0.3825\n",
            "Epoch 14 Loss 2.3744 Accuracy 0.4073\n",
            "Time taken for 1 epoch: 14.071186542510986 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.2492 Accuracy 0.4179\n",
            "Saving checkpoint for epoch 15 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-3\n",
            "Epoch 15 Loss 2.1238 Accuracy 0.4313\n",
            "Time taken for 1 epoch: 14.055440902709961 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 2.0208 Accuracy 0.4435\n",
            "Epoch 16 Loss 1.9529 Accuracy 0.4427\n",
            "Time taken for 1 epoch: 13.918891191482544 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.9238 Accuracy 0.4479\n",
            "Epoch 17 Loss 1.8335 Accuracy 0.4543\n",
            "Time taken for 1 epoch: 13.963773727416992 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.7250 Accuracy 0.4855\n",
            "Epoch 18 Loss 1.7473 Accuracy 0.4670\n",
            "Time taken for 1 epoch: 13.763843774795532 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.7001 Accuracy 0.4782\n",
            "Epoch 19 Loss 1.6813 Accuracy 0.4798\n",
            "Time taken for 1 epoch: 13.881187438964844 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.6325 Accuracy 0.4921\n",
            "Saving checkpoint for epoch 20 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-4\n",
            "Epoch 20 Loss 1.6231 Accuracy 0.4920\n",
            "Time taken for 1 epoch: 14.323596715927124 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.5966 Accuracy 0.4872\n",
            "Epoch 21 Loss 1.5758 Accuracy 0.5032\n",
            "Time taken for 1 epoch: 14.06230354309082 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.5952 Accuracy 0.4929\n",
            "Epoch 22 Loss 1.5285 Accuracy 0.5161\n",
            "Time taken for 1 epoch: 13.711594343185425 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.5173 Accuracy 0.5116\n",
            "Epoch 23 Loss 1.4879 Accuracy 0.5274\n",
            "Time taken for 1 epoch: 13.908786296844482 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.4809 Accuracy 0.5227\n",
            "Epoch 24 Loss 1.4547 Accuracy 0.5360\n",
            "Time taken for 1 epoch: 13.941237688064575 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.4470 Accuracy 0.5420\n",
            "Saving checkpoint for epoch 25 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-5\n",
            "Epoch 25 Loss 1.4182 Accuracy 0.5462\n",
            "Time taken for 1 epoch: 14.326165437698364 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.4065 Accuracy 0.5554\n",
            "Epoch 26 Loss 1.3862 Accuracy 0.5564\n",
            "Time taken for 1 epoch: 13.944384336471558 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.3287 Accuracy 0.5812\n",
            "Epoch 27 Loss 1.3534 Accuracy 0.5664\n",
            "Time taken for 1 epoch: 13.859133005142212 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.3127 Accuracy 0.5877\n",
            "Epoch 28 Loss 1.3261 Accuracy 0.5742\n",
            "Time taken for 1 epoch: 13.853686571121216 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 1.2805 Accuracy 0.5944\n",
            "Epoch 29 Loss 1.2937 Accuracy 0.5839\n",
            "Time taken for 1 epoch: 13.993417024612427 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.2837 Accuracy 0.5899\n",
            "Saving checkpoint for epoch 30 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-6\n",
            "Epoch 30 Loss 1.2669 Accuracy 0.5928\n",
            "Time taken for 1 epoch: 14.125810623168945 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.1831 Accuracy 0.6169\n",
            "Epoch 31 Loss 1.2392 Accuracy 0.6014\n",
            "Time taken for 1 epoch: 13.905577659606934 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 1.2158 Accuracy 0.6144\n",
            "Epoch 32 Loss 1.2185 Accuracy 0.6080\n",
            "Time taken for 1 epoch: 13.859744548797607 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 1.1603 Accuracy 0.6335\n",
            "Epoch 33 Loss 1.1901 Accuracy 0.6162\n",
            "Time taken for 1 epoch: 14.116874933242798 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 1.2019 Accuracy 0.6085\n",
            "Epoch 34 Loss 1.1704 Accuracy 0.6224\n",
            "Time taken for 1 epoch: 13.91430377960205 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 1.1953 Accuracy 0.6172\n",
            "Saving checkpoint for epoch 35 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-7\n",
            "Epoch 35 Loss 1.1477 Accuracy 0.6295\n",
            "Time taken for 1 epoch: 14.204493045806885 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 1.0908 Accuracy 0.6545\n",
            "Epoch 36 Loss 1.1243 Accuracy 0.6359\n",
            "Time taken for 1 epoch: 14.00606918334961 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 1.0361 Accuracy 0.6622\n",
            "Epoch 37 Loss 1.1023 Accuracy 0.6432\n",
            "Time taken for 1 epoch: 13.974389791488647 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 1.0587 Accuracy 0.6604\n",
            "Epoch 38 Loss 1.0824 Accuracy 0.6497\n",
            "Time taken for 1 epoch: 13.800700426101685 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 1.0540 Accuracy 0.6625\n",
            "Epoch 39 Loss 1.0615 Accuracy 0.6563\n",
            "Time taken for 1 epoch: 13.864678859710693 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 1.0133 Accuracy 0.6751\n",
            "Saving checkpoint for epoch 40 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-8\n",
            "Epoch 40 Loss 1.0431 Accuracy 0.6609\n",
            "Time taken for 1 epoch: 14.105645179748535 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 1.0307 Accuracy 0.6704\n",
            "Epoch 41 Loss 1.0284 Accuracy 0.6670\n",
            "Time taken for 1 epoch: 14.163119077682495 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.9867 Accuracy 0.6862\n",
            "Epoch 42 Loss 1.0057 Accuracy 0.6734\n",
            "Time taken for 1 epoch: 13.817511081695557 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.9510 Accuracy 0.6870\n",
            "Epoch 43 Loss 0.9882 Accuracy 0.6792\n",
            "Time taken for 1 epoch: 13.70974087715149 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.9190 Accuracy 0.6986\n",
            "Epoch 44 Loss 0.9710 Accuracy 0.6844\n",
            "Time taken for 1 epoch: 13.802669286727905 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.9721 Accuracy 0.6802\n",
            "Saving checkpoint for epoch 45 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-9\n",
            "Epoch 45 Loss 0.9541 Accuracy 0.6894\n",
            "Time taken for 1 epoch: 14.049283266067505 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.9345 Accuracy 0.6906\n",
            "Epoch 46 Loss 0.9402 Accuracy 0.6941\n",
            "Time taken for 1 epoch: 13.868837594985962 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.8849 Accuracy 0.7243\n",
            "Epoch 47 Loss 0.9195 Accuracy 0.7003\n",
            "Time taken for 1 epoch: 14.142762422561646 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.9319 Accuracy 0.6905\n",
            "Epoch 48 Loss 0.9068 Accuracy 0.7051\n",
            "Time taken for 1 epoch: 14.229949235916138 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.8412 Accuracy 0.7269\n",
            "Epoch 49 Loss 0.8890 Accuracy 0.7108\n",
            "Time taken for 1 epoch: 14.111085891723633 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.9198 Accuracy 0.7010\n",
            "Saving checkpoint for epoch 50 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-10\n",
            "Epoch 50 Loss 0.8763 Accuracy 0.7145\n",
            "Time taken for 1 epoch: 13.924575805664062 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.8405 Accuracy 0.7217\n",
            "Epoch 51 Loss 0.8573 Accuracy 0.7201\n",
            "Time taken for 1 epoch: 13.788003206253052 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.8351 Accuracy 0.7294\n",
            "Epoch 52 Loss 0.8458 Accuracy 0.7234\n",
            "Time taken for 1 epoch: 13.618091583251953 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.8035 Accuracy 0.7450\n",
            "Epoch 53 Loss 0.8310 Accuracy 0.7281\n",
            "Time taken for 1 epoch: 13.91859745979309 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.7977 Accuracy 0.7359\n",
            "Epoch 54 Loss 0.8173 Accuracy 0.7320\n",
            "Time taken for 1 epoch: 13.82685375213623 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.7316 Accuracy 0.7546\n",
            "Saving checkpoint for epoch 55 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-11\n",
            "Epoch 55 Loss 0.7997 Accuracy 0.7376\n",
            "Time taken for 1 epoch: 14.32207727432251 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.7754 Accuracy 0.7434\n",
            "Epoch 56 Loss 0.7876 Accuracy 0.7417\n",
            "Time taken for 1 epoch: 13.739268064498901 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.7772 Accuracy 0.7510\n",
            "Epoch 57 Loss 0.7771 Accuracy 0.7445\n",
            "Time taken for 1 epoch: 13.796618938446045 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.7146 Accuracy 0.7618\n",
            "Epoch 58 Loss 0.7614 Accuracy 0.7495\n",
            "Time taken for 1 epoch: 13.759052515029907 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.7269 Accuracy 0.7550\n",
            "Epoch 59 Loss 0.7490 Accuracy 0.7538\n",
            "Time taken for 1 epoch: 13.91236424446106 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.6847 Accuracy 0.7751\n",
            "Saving checkpoint for epoch 60 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-12\n",
            "Epoch 60 Loss 0.7374 Accuracy 0.7571\n",
            "Time taken for 1 epoch: 14.068872928619385 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.7282 Accuracy 0.7592\n",
            "Epoch 61 Loss 0.7202 Accuracy 0.7628\n",
            "Time taken for 1 epoch: 13.990490913391113 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.6676 Accuracy 0.7816\n",
            "Epoch 62 Loss 0.7030 Accuracy 0.7690\n",
            "Time taken for 1 epoch: 13.87319803237915 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.6953 Accuracy 0.7809\n",
            "Epoch 63 Loss 0.6986 Accuracy 0.7703\n",
            "Time taken for 1 epoch: 14.051029205322266 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.5988 Accuracy 0.7992\n",
            "Epoch 64 Loss 0.6888 Accuracy 0.7736\n",
            "Time taken for 1 epoch: 13.89836072921753 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.6180 Accuracy 0.7911\n",
            "Saving checkpoint for epoch 65 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-13\n",
            "Epoch 65 Loss 0.6741 Accuracy 0.7772\n",
            "Time taken for 1 epoch: 14.424824953079224 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.5535 Accuracy 0.8143\n",
            "Epoch 66 Loss 0.6668 Accuracy 0.7794\n",
            "Time taken for 1 epoch: 13.954172372817993 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.6052 Accuracy 0.7990\n",
            "Epoch 67 Loss 0.6476 Accuracy 0.7848\n",
            "Time taken for 1 epoch: 13.84557032585144 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.6076 Accuracy 0.7940\n",
            "Epoch 68 Loss 0.6413 Accuracy 0.7880\n",
            "Time taken for 1 epoch: 13.842091083526611 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.5889 Accuracy 0.8024\n",
            "Epoch 69 Loss 0.6300 Accuracy 0.7902\n",
            "Time taken for 1 epoch: 14.063701868057251 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.5534 Accuracy 0.8138\n",
            "Saving checkpoint for epoch 70 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-14\n",
            "Epoch 70 Loss 0.6205 Accuracy 0.7935\n",
            "Time taken for 1 epoch: 14.150049924850464 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.5667 Accuracy 0.8111\n",
            "Epoch 71 Loss 0.6054 Accuracy 0.7981\n",
            "Time taken for 1 epoch: 13.950756311416626 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.5228 Accuracy 0.8229\n",
            "Epoch 72 Loss 0.5991 Accuracy 0.7994\n",
            "Time taken for 1 epoch: 13.971660614013672 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.4587 Accuracy 0.8524\n",
            "Epoch 73 Loss 0.5851 Accuracy 0.8052\n",
            "Time taken for 1 epoch: 13.769481897354126 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.5420 Accuracy 0.8189\n",
            "Epoch 74 Loss 0.5712 Accuracy 0.8097\n",
            "Time taken for 1 epoch: 13.776898384094238 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.4686 Accuracy 0.8504\n",
            "Saving checkpoint for epoch 75 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-15\n",
            "Epoch 75 Loss 0.5677 Accuracy 0.8101\n",
            "Time taken for 1 epoch: 14.090264320373535 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.5128 Accuracy 0.8235\n",
            "Epoch 76 Loss 0.5572 Accuracy 0.8134\n",
            "Time taken for 1 epoch: 13.743674755096436 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.4288 Accuracy 0.8625\n",
            "Epoch 77 Loss 0.5484 Accuracy 0.8160\n",
            "Time taken for 1 epoch: 13.875138998031616 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.5243 Accuracy 0.8253\n",
            "Epoch 78 Loss 0.5388 Accuracy 0.8197\n",
            "Time taken for 1 epoch: 13.952674150466919 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.4693 Accuracy 0.8395\n",
            "Epoch 79 Loss 0.5301 Accuracy 0.8216\n",
            "Time taken for 1 epoch: 13.899436235427856 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.4481 Accuracy 0.8489\n",
            "Saving checkpoint for epoch 80 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-16\n",
            "Epoch 80 Loss 0.5174 Accuracy 0.8253\n",
            "Time taken for 1 epoch: 14.189019441604614 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.4878 Accuracy 0.8390\n",
            "Epoch 81 Loss 0.5091 Accuracy 0.8286\n",
            "Time taken for 1 epoch: 13.9274263381958 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.4847 Accuracy 0.8359\n",
            "Epoch 82 Loss 0.5013 Accuracy 0.8312\n",
            "Time taken for 1 epoch: 13.806324243545532 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.4429 Accuracy 0.8450\n",
            "Epoch 83 Loss 0.4999 Accuracy 0.8308\n",
            "Time taken for 1 epoch: 13.771885395050049 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.4434 Accuracy 0.8509\n",
            "Epoch 84 Loss 0.4851 Accuracy 0.8350\n",
            "Time taken for 1 epoch: 13.90557312965393 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.4640 Accuracy 0.8381\n",
            "Saving checkpoint for epoch 85 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-17\n",
            "Epoch 85 Loss 0.4788 Accuracy 0.8386\n",
            "Time taken for 1 epoch: 13.8590989112854 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.4109 Accuracy 0.8587\n",
            "Epoch 86 Loss 0.4691 Accuracy 0.8407\n",
            "Time taken for 1 epoch: 13.655472040176392 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.4138 Accuracy 0.8521\n",
            "Epoch 87 Loss 0.4695 Accuracy 0.8409\n",
            "Time taken for 1 epoch: 13.933777093887329 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.4174 Accuracy 0.8633\n",
            "Epoch 88 Loss 0.4577 Accuracy 0.8443\n",
            "Time taken for 1 epoch: 13.79243278503418 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.4155 Accuracy 0.8586\n",
            "Epoch 89 Loss 0.4521 Accuracy 0.8472\n",
            "Time taken for 1 epoch: 13.829112768173218 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.4308 Accuracy 0.8510\n",
            "Saving checkpoint for epoch 90 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-18\n",
            "Epoch 90 Loss 0.4469 Accuracy 0.8482\n",
            "Time taken for 1 epoch: 14.075968980789185 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.3732 Accuracy 0.8787\n",
            "Epoch 91 Loss 0.4367 Accuracy 0.8509\n",
            "Time taken for 1 epoch: 13.879771947860718 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.3931 Accuracy 0.8702\n",
            "Epoch 92 Loss 0.4338 Accuracy 0.8528\n",
            "Time taken for 1 epoch: 13.788278102874756 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.3658 Accuracy 0.8767\n",
            "Epoch 93 Loss 0.4216 Accuracy 0.8565\n",
            "Time taken for 1 epoch: 13.786492109298706 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.3463 Accuracy 0.8858\n",
            "Epoch 94 Loss 0.4266 Accuracy 0.8549\n",
            "Time taken for 1 epoch: 13.77750277519226 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.3837 Accuracy 0.8678\n",
            "Saving checkpoint for epoch 95 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-19\n",
            "Epoch 95 Loss 0.4174 Accuracy 0.8570\n",
            "Time taken for 1 epoch: 14.174289226531982 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.4165 Accuracy 0.8532\n",
            "Epoch 96 Loss 0.4098 Accuracy 0.8593\n",
            "Time taken for 1 epoch: 13.982798099517822 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.3887 Accuracy 0.8713\n",
            "Epoch 97 Loss 0.4070 Accuracy 0.8611\n",
            "Time taken for 1 epoch: 13.773099184036255 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.3483 Accuracy 0.8785\n",
            "Epoch 98 Loss 0.3942 Accuracy 0.8646\n",
            "Time taken for 1 epoch: 13.791417837142944 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.3877 Accuracy 0.8732\n",
            "Epoch 99 Loss 0.3953 Accuracy 0.8641\n",
            "Time taken for 1 epoch: 13.668709754943848 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.3167 Accuracy 0.8858\n",
            "Saving checkpoint for epoch 100 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-20\n",
            "Epoch 100 Loss 0.3877 Accuracy 0.8672\n",
            "Time taken for 1 epoch: 14.040534496307373 secs\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.3271 Accuracy 0.8869\n",
            "Epoch 101 Loss 0.3764 Accuracy 0.8707\n",
            "Time taken for 1 epoch: 13.73763394355774 secs\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.3476 Accuracy 0.8796\n",
            "Epoch 102 Loss 0.3743 Accuracy 0.8705\n",
            "Time taken for 1 epoch: 13.706395149230957 secs\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.3290 Accuracy 0.8901\n",
            "Epoch 103 Loss 0.3637 Accuracy 0.8749\n",
            "Time taken for 1 epoch: 13.768386840820312 secs\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.3563 Accuracy 0.8757\n",
            "Epoch 104 Loss 0.3604 Accuracy 0.8762\n",
            "Time taken for 1 epoch: 13.575267553329468 secs\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.2972 Accuracy 0.8934\n",
            "Saving checkpoint for epoch 105 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-21\n",
            "Epoch 105 Loss 0.3520 Accuracy 0.8775\n",
            "Time taken for 1 epoch: 13.949569940567017 secs\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.3505 Accuracy 0.8793\n",
            "Epoch 106 Loss 0.3459 Accuracy 0.8803\n",
            "Time taken for 1 epoch: 13.73637318611145 secs\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.3442 Accuracy 0.8760\n",
            "Epoch 107 Loss 0.3381 Accuracy 0.8828\n",
            "Time taken for 1 epoch: 13.588316202163696 secs\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.3003 Accuracy 0.8973\n",
            "Epoch 108 Loss 0.3296 Accuracy 0.8867\n",
            "Time taken for 1 epoch: 13.741624116897583 secs\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.2747 Accuracy 0.9033\n",
            "Epoch 109 Loss 0.3231 Accuracy 0.8878\n",
            "Time taken for 1 epoch: 13.761195421218872 secs\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.2935 Accuracy 0.9002\n",
            "Saving checkpoint for epoch 110 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-22\n",
            "Epoch 110 Loss 0.3157 Accuracy 0.8916\n",
            "Time taken for 1 epoch: 13.844762086868286 secs\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.2506 Accuracy 0.9147\n",
            "Epoch 111 Loss 0.3065 Accuracy 0.8938\n",
            "Time taken for 1 epoch: 13.672977447509766 secs\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.2914 Accuracy 0.9021\n",
            "Epoch 112 Loss 0.2987 Accuracy 0.8967\n",
            "Time taken for 1 epoch: 13.671213388442993 secs\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.2436 Accuracy 0.9183\n",
            "Epoch 113 Loss 0.2935 Accuracy 0.8976\n",
            "Time taken for 1 epoch: 13.856167793273926 secs\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.2528 Accuracy 0.9153\n",
            "Epoch 114 Loss 0.2904 Accuracy 0.9000\n",
            "Time taken for 1 epoch: 13.712538003921509 secs\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.2519 Accuracy 0.9165\n",
            "Saving checkpoint for epoch 115 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-23\n",
            "Epoch 115 Loss 0.2904 Accuracy 0.8985\n",
            "Time taken for 1 epoch: 14.131843090057373 secs\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.2624 Accuracy 0.9101\n",
            "Epoch 116 Loss 0.2798 Accuracy 0.9021\n",
            "Time taken for 1 epoch: 13.658782720565796 secs\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.2288 Accuracy 0.9204\n",
            "Epoch 117 Loss 0.2716 Accuracy 0.9059\n",
            "Time taken for 1 epoch: 13.691318035125732 secs\n",
            "\n",
            "Epoch 118 Batch 0 Loss 0.1928 Accuracy 0.9314\n",
            "Epoch 118 Loss 0.2677 Accuracy 0.9068\n",
            "Time taken for 1 epoch: 13.80315375328064 secs\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.2441 Accuracy 0.9112\n",
            "Epoch 119 Loss 0.2703 Accuracy 0.9063\n",
            "Time taken for 1 epoch: 13.742625713348389 secs\n",
            "\n",
            "Epoch 120 Batch 0 Loss 0.2673 Accuracy 0.9105\n",
            "Saving checkpoint for epoch 120 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-24\n",
            "Epoch 120 Loss 0.2582 Accuracy 0.9105\n",
            "Time taken for 1 epoch: 13.928659677505493 secs\n",
            "\n",
            "Epoch 121 Batch 0 Loss 0.2245 Accuracy 0.9163\n",
            "Epoch 121 Loss 0.2562 Accuracy 0.9111\n",
            "Time taken for 1 epoch: 13.93547797203064 secs\n",
            "\n",
            "Epoch 122 Batch 0 Loss 0.2130 Accuracy 0.9259\n",
            "Epoch 122 Loss 0.2517 Accuracy 0.9125\n",
            "Time taken for 1 epoch: 13.918188571929932 secs\n",
            "\n",
            "Epoch 123 Batch 0 Loss 0.2165 Accuracy 0.9210\n",
            "Epoch 123 Loss 0.2459 Accuracy 0.9145\n",
            "Time taken for 1 epoch: 13.806950569152832 secs\n",
            "\n",
            "Epoch 124 Batch 0 Loss 0.1977 Accuracy 0.9312\n",
            "Epoch 124 Loss 0.2376 Accuracy 0.9172\n",
            "Time taken for 1 epoch: 13.736958265304565 secs\n",
            "\n",
            "Epoch 125 Batch 0 Loss 0.2139 Accuracy 0.9234\n",
            "Saving checkpoint for epoch 125 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-25\n",
            "Epoch 125 Loss 0.2382 Accuracy 0.9165\n",
            "Time taken for 1 epoch: 14.031635761260986 secs\n",
            "\n",
            "Epoch 126 Batch 0 Loss 0.1898 Accuracy 0.9332\n",
            "Epoch 126 Loss 0.2313 Accuracy 0.9192\n",
            "Time taken for 1 epoch: 13.884780645370483 secs\n",
            "\n",
            "Epoch 127 Batch 0 Loss 0.1799 Accuracy 0.9356\n",
            "Epoch 127 Loss 0.2291 Accuracy 0.9197\n",
            "Time taken for 1 epoch: 13.720117568969727 secs\n",
            "\n",
            "Epoch 128 Batch 0 Loss 0.2110 Accuracy 0.9241\n",
            "Epoch 128 Loss 0.2263 Accuracy 0.9211\n",
            "Time taken for 1 epoch: 13.872364044189453 secs\n",
            "\n",
            "Epoch 129 Batch 0 Loss 0.2251 Accuracy 0.9225\n",
            "Epoch 129 Loss 0.2200 Accuracy 0.9231\n",
            "Time taken for 1 epoch: 13.760911703109741 secs\n",
            "\n",
            "Epoch 130 Batch 0 Loss 0.1831 Accuracy 0.9349\n",
            "Saving checkpoint for epoch 130 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-26\n",
            "Epoch 130 Loss 0.2232 Accuracy 0.9221\n",
            "Time taken for 1 epoch: 13.894368410110474 secs\n",
            "\n",
            "Epoch 131 Batch 0 Loss 0.1838 Accuracy 0.9331\n",
            "Epoch 131 Loss 0.2114 Accuracy 0.9260\n",
            "Time taken for 1 epoch: 13.863612651824951 secs\n",
            "\n",
            "Epoch 132 Batch 0 Loss 0.1784 Accuracy 0.9364\n",
            "Epoch 132 Loss 0.2102 Accuracy 0.9267\n",
            "Time taken for 1 epoch: 13.706779956817627 secs\n",
            "\n",
            "Epoch 133 Batch 0 Loss 0.2057 Accuracy 0.9224\n",
            "Epoch 133 Loss 0.2081 Accuracy 0.9268\n",
            "Time taken for 1 epoch: 13.67613935470581 secs\n",
            "\n",
            "Epoch 134 Batch 0 Loss 0.1699 Accuracy 0.9402\n",
            "Epoch 134 Loss 0.2017 Accuracy 0.9294\n",
            "Time taken for 1 epoch: 13.71966004371643 secs\n",
            "\n",
            "Epoch 135 Batch 0 Loss 0.1821 Accuracy 0.9372\n",
            "Saving checkpoint for epoch 135 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-27\n",
            "Epoch 135 Loss 0.1967 Accuracy 0.9317\n",
            "Time taken for 1 epoch: 14.214972496032715 secs\n",
            "\n",
            "Epoch 136 Batch 0 Loss 0.1623 Accuracy 0.9447\n",
            "Epoch 136 Loss 0.1994 Accuracy 0.9308\n",
            "Time taken for 1 epoch: 13.72253680229187 secs\n",
            "\n",
            "Epoch 137 Batch 0 Loss 0.1611 Accuracy 0.9425\n",
            "Epoch 137 Loss 0.1926 Accuracy 0.9329\n",
            "Time taken for 1 epoch: 13.756958246231079 secs\n",
            "\n",
            "Epoch 138 Batch 0 Loss 0.1715 Accuracy 0.9420\n",
            "Epoch 138 Loss 0.1854 Accuracy 0.9350\n",
            "Time taken for 1 epoch: 13.7782301902771 secs\n",
            "\n",
            "Epoch 139 Batch 0 Loss 0.1661 Accuracy 0.9429\n",
            "Epoch 139 Loss 0.1892 Accuracy 0.9338\n",
            "Time taken for 1 epoch: 13.775283098220825 secs\n",
            "\n",
            "Epoch 140 Batch 0 Loss 0.1933 Accuracy 0.9326\n",
            "Saving checkpoint for epoch 140 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-28\n",
            "Epoch 140 Loss 0.1835 Accuracy 0.9361\n",
            "Time taken for 1 epoch: 13.932381629943848 secs\n",
            "\n",
            "Epoch 141 Batch 0 Loss 0.1512 Accuracy 0.9482\n",
            "Epoch 141 Loss 0.1819 Accuracy 0.9364\n",
            "Time taken for 1 epoch: 13.811326026916504 secs\n",
            "\n",
            "Epoch 142 Batch 0 Loss 0.1611 Accuracy 0.9443\n",
            "Epoch 142 Loss 0.1851 Accuracy 0.9364\n",
            "Time taken for 1 epoch: 13.85215449333191 secs\n",
            "\n",
            "Epoch 143 Batch 0 Loss 0.1559 Accuracy 0.9482\n",
            "Epoch 143 Loss 0.1735 Accuracy 0.9398\n",
            "Time taken for 1 epoch: 13.666375160217285 secs\n",
            "\n",
            "Epoch 144 Batch 0 Loss 0.1517 Accuracy 0.9444\n",
            "Epoch 144 Loss 0.1728 Accuracy 0.9399\n",
            "Time taken for 1 epoch: 13.75302767753601 secs\n",
            "\n",
            "Epoch 145 Batch 0 Loss 0.1431 Accuracy 0.9507\n",
            "Saving checkpoint for epoch 145 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-29\n",
            "Epoch 145 Loss 0.1744 Accuracy 0.9400\n",
            "Time taken for 1 epoch: 14.118305206298828 secs\n",
            "\n",
            "Epoch 146 Batch 0 Loss 0.1345 Accuracy 0.9552\n",
            "Epoch 146 Loss 0.1713 Accuracy 0.9402\n",
            "Time taken for 1 epoch: 13.688033103942871 secs\n",
            "\n",
            "Epoch 147 Batch 0 Loss 0.1347 Accuracy 0.9543\n",
            "Epoch 147 Loss 0.1657 Accuracy 0.9424\n",
            "Time taken for 1 epoch: 13.715110778808594 secs\n",
            "\n",
            "Epoch 148 Batch 0 Loss 0.1522 Accuracy 0.9530\n",
            "Epoch 148 Loss 0.1636 Accuracy 0.9434\n",
            "Time taken for 1 epoch: 13.907888889312744 secs\n",
            "\n",
            "Epoch 149 Batch 0 Loss 0.1606 Accuracy 0.9432\n",
            "Epoch 149 Loss 0.1601 Accuracy 0.9439\n",
            "Time taken for 1 epoch: 13.788703680038452 secs\n",
            "\n",
            "Epoch 150 Batch 0 Loss 0.1351 Accuracy 0.9524\n",
            "Saving checkpoint for epoch 150 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-30\n",
            "Epoch 150 Loss 0.1616 Accuracy 0.9439\n",
            "Time taken for 1 epoch: 13.906745433807373 secs\n",
            "\n",
            "Epoch 151 Batch 0 Loss 0.1565 Accuracy 0.9442\n",
            "Epoch 151 Loss 0.1617 Accuracy 0.9436\n",
            "Time taken for 1 epoch: 13.71802020072937 secs\n",
            "\n",
            "Epoch 152 Batch 0 Loss 0.1431 Accuracy 0.9508\n",
            "Epoch 152 Loss 0.1543 Accuracy 0.9467\n",
            "Time taken for 1 epoch: 13.675219297409058 secs\n",
            "\n",
            "Epoch 153 Batch 0 Loss 0.1197 Accuracy 0.9608\n",
            "Epoch 153 Loss 0.1522 Accuracy 0.9476\n",
            "Time taken for 1 epoch: 13.794346809387207 secs\n",
            "\n",
            "Epoch 154 Batch 0 Loss 0.1220 Accuracy 0.9569\n",
            "Epoch 154 Loss 0.1511 Accuracy 0.9472\n",
            "Time taken for 1 epoch: 13.895525455474854 secs\n",
            "\n",
            "Epoch 155 Batch 0 Loss 0.1211 Accuracy 0.9592\n",
            "Saving checkpoint for epoch 155 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-31\n",
            "Epoch 155 Loss 0.1485 Accuracy 0.9491\n",
            "Time taken for 1 epoch: 14.044695377349854 secs\n",
            "\n",
            "Epoch 156 Batch 0 Loss 0.1256 Accuracy 0.9556\n",
            "Epoch 156 Loss 0.1461 Accuracy 0.9487\n",
            "Time taken for 1 epoch: 13.686256647109985 secs\n",
            "\n",
            "Epoch 157 Batch 0 Loss 0.1264 Accuracy 0.9546\n",
            "Epoch 157 Loss 0.1500 Accuracy 0.9481\n",
            "Time taken for 1 epoch: 13.715714693069458 secs\n",
            "\n",
            "Epoch 158 Batch 0 Loss 0.1099 Accuracy 0.9636\n",
            "Epoch 158 Loss 0.1447 Accuracy 0.9500\n",
            "Time taken for 1 epoch: 13.8718421459198 secs\n",
            "\n",
            "Epoch 159 Batch 0 Loss 0.1090 Accuracy 0.9664\n",
            "Epoch 159 Loss 0.1401 Accuracy 0.9514\n",
            "Time taken for 1 epoch: 13.660498857498169 secs\n",
            "\n",
            "Epoch 160 Batch 0 Loss 0.1171 Accuracy 0.9599\n",
            "Saving checkpoint for epoch 160 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-32\n",
            "Epoch 160 Loss 0.1386 Accuracy 0.9515\n",
            "Time taken for 1 epoch: 13.999205112457275 secs\n",
            "\n",
            "Epoch 161 Batch 0 Loss 0.1095 Accuracy 0.9602\n",
            "Epoch 161 Loss 0.1399 Accuracy 0.9513\n",
            "Time taken for 1 epoch: 13.9919753074646 secs\n",
            "\n",
            "Epoch 162 Batch 0 Loss 0.1207 Accuracy 0.9529\n",
            "Epoch 162 Loss 0.1359 Accuracy 0.9524\n",
            "Time taken for 1 epoch: 13.69450831413269 secs\n",
            "\n",
            "Epoch 163 Batch 0 Loss 0.1089 Accuracy 0.9620\n",
            "Epoch 163 Loss 0.1357 Accuracy 0.9530\n",
            "Time taken for 1 epoch: 13.606705665588379 secs\n",
            "\n",
            "Epoch 164 Batch 0 Loss 0.1148 Accuracy 0.9621\n",
            "Epoch 164 Loss 0.1327 Accuracy 0.9540\n",
            "Time taken for 1 epoch: 13.642455339431763 secs\n",
            "\n",
            "Epoch 165 Batch 0 Loss 0.0988 Accuracy 0.9661\n",
            "Saving checkpoint for epoch 165 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-33\n",
            "Epoch 165 Loss 0.1288 Accuracy 0.9555\n",
            "Time taken for 1 epoch: 14.006776332855225 secs\n",
            "\n",
            "Epoch 166 Batch 0 Loss 0.1111 Accuracy 0.9601\n",
            "Epoch 166 Loss 0.1328 Accuracy 0.9545\n",
            "Time taken for 1 epoch: 13.599063158035278 secs\n",
            "\n",
            "Epoch 167 Batch 0 Loss 0.1321 Accuracy 0.9555\n",
            "Epoch 167 Loss 0.1316 Accuracy 0.9536\n",
            "Time taken for 1 epoch: 13.89735460281372 secs\n",
            "\n",
            "Epoch 168 Batch 0 Loss 0.1004 Accuracy 0.9641\n",
            "Epoch 168 Loss 0.1265 Accuracy 0.9557\n",
            "Time taken for 1 epoch: 13.676904678344727 secs\n",
            "\n",
            "Epoch 169 Batch 0 Loss 0.0973 Accuracy 0.9657\n",
            "Epoch 169 Loss 0.1256 Accuracy 0.9560\n",
            "Time taken for 1 epoch: 13.91871428489685 secs\n",
            "\n",
            "Epoch 170 Batch 0 Loss 0.1133 Accuracy 0.9632\n",
            "Saving checkpoint for epoch 170 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-34\n",
            "Epoch 170 Loss 0.1234 Accuracy 0.9576\n",
            "Time taken for 1 epoch: 13.831387519836426 secs\n",
            "\n",
            "Epoch 171 Batch 0 Loss 0.1002 Accuracy 0.9644\n",
            "Epoch 171 Loss 0.1208 Accuracy 0.9582\n",
            "Time taken for 1 epoch: 13.843736171722412 secs\n",
            "\n",
            "Epoch 172 Batch 0 Loss 0.1001 Accuracy 0.9626\n",
            "Epoch 172 Loss 0.1219 Accuracy 0.9571\n",
            "Time taken for 1 epoch: 13.642034769058228 secs\n",
            "\n",
            "Epoch 173 Batch 0 Loss 0.0972 Accuracy 0.9659\n",
            "Epoch 173 Loss 0.1148 Accuracy 0.9606\n",
            "Time taken for 1 epoch: 13.601493835449219 secs\n",
            "\n",
            "Epoch 174 Batch 0 Loss 0.1072 Accuracy 0.9641\n",
            "Epoch 174 Loss 0.1190 Accuracy 0.9589\n",
            "Time taken for 1 epoch: 13.714410305023193 secs\n",
            "\n",
            "Epoch 175 Batch 0 Loss 0.1137 Accuracy 0.9626\n",
            "Saving checkpoint for epoch 175 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-35\n",
            "Epoch 175 Loss 0.1205 Accuracy 0.9588\n",
            "Time taken for 1 epoch: 13.8687744140625 secs\n",
            "\n",
            "Epoch 176 Batch 0 Loss 0.1157 Accuracy 0.9588\n",
            "Epoch 176 Loss 0.1166 Accuracy 0.9593\n",
            "Time taken for 1 epoch: 13.678508758544922 secs\n",
            "\n",
            "Epoch 177 Batch 0 Loss 0.1044 Accuracy 0.9637\n",
            "Epoch 177 Loss 0.1157 Accuracy 0.9602\n",
            "Time taken for 1 epoch: 13.824216604232788 secs\n",
            "\n",
            "Epoch 178 Batch 0 Loss 0.0975 Accuracy 0.9662\n",
            "Epoch 178 Loss 0.1129 Accuracy 0.9607\n",
            "Time taken for 1 epoch: 13.719630479812622 secs\n",
            "\n",
            "Epoch 179 Batch 0 Loss 0.0998 Accuracy 0.9664\n",
            "Epoch 179 Loss 0.1165 Accuracy 0.9593\n",
            "Time taken for 1 epoch: 13.650668621063232 secs\n",
            "\n",
            "Epoch 180 Batch 0 Loss 0.1143 Accuracy 0.9644\n",
            "Saving checkpoint for epoch 180 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-36\n",
            "Epoch 180 Loss 0.1137 Accuracy 0.9607\n",
            "Time taken for 1 epoch: 14.064560890197754 secs\n",
            "\n",
            "Epoch 181 Batch 0 Loss 0.0953 Accuracy 0.9653\n",
            "Epoch 181 Loss 0.1096 Accuracy 0.9620\n",
            "Time taken for 1 epoch: 13.972370147705078 secs\n",
            "\n",
            "Epoch 182 Batch 0 Loss 0.0855 Accuracy 0.9712\n",
            "Epoch 182 Loss 0.1079 Accuracy 0.9624\n",
            "Time taken for 1 epoch: 13.626037836074829 secs\n",
            "\n",
            "Epoch 183 Batch 0 Loss 0.1002 Accuracy 0.9668\n",
            "Epoch 183 Loss 0.1103 Accuracy 0.9613\n",
            "Time taken for 1 epoch: 13.77043604850769 secs\n",
            "\n",
            "Epoch 184 Batch 0 Loss 0.0930 Accuracy 0.9668\n",
            "Epoch 184 Loss 0.1100 Accuracy 0.9617\n",
            "Time taken for 1 epoch: 13.66628122329712 secs\n",
            "\n",
            "Epoch 185 Batch 0 Loss 0.0952 Accuracy 0.9655\n",
            "Saving checkpoint for epoch 185 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-37\n",
            "Epoch 185 Loss 0.1042 Accuracy 0.9638\n",
            "Time taken for 1 epoch: 13.914743423461914 secs\n",
            "\n",
            "Epoch 186 Batch 0 Loss 0.0910 Accuracy 0.9664\n",
            "Epoch 186 Loss 0.1060 Accuracy 0.9627\n",
            "Time taken for 1 epoch: 13.594559669494629 secs\n",
            "\n",
            "Epoch 187 Batch 0 Loss 0.0921 Accuracy 0.9664\n",
            "Epoch 187 Loss 0.1039 Accuracy 0.9641\n",
            "Time taken for 1 epoch: 13.875425100326538 secs\n",
            "\n",
            "Epoch 188 Batch 0 Loss 0.0887 Accuracy 0.9679\n",
            "Epoch 188 Loss 0.1050 Accuracy 0.9632\n",
            "Time taken for 1 epoch: 13.73543930053711 secs\n",
            "\n",
            "Epoch 189 Batch 0 Loss 0.0676 Accuracy 0.9763\n",
            "Epoch 189 Loss 0.1027 Accuracy 0.9646\n",
            "Time taken for 1 epoch: 13.979617595672607 secs\n",
            "\n",
            "Epoch 190 Batch 0 Loss 0.0841 Accuracy 0.9713\n",
            "Saving checkpoint for epoch 190 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-38\n",
            "Epoch 190 Loss 0.0984 Accuracy 0.9663\n",
            "Time taken for 1 epoch: 14.184059381484985 secs\n",
            "\n",
            "Epoch 191 Batch 0 Loss 0.0853 Accuracy 0.9676\n",
            "Epoch 191 Loss 0.0958 Accuracy 0.9668\n",
            "Time taken for 1 epoch: 13.564894199371338 secs\n",
            "\n",
            "Epoch 192 Batch 0 Loss 0.0863 Accuracy 0.9678\n",
            "Epoch 192 Loss 0.1011 Accuracy 0.9649\n",
            "Time taken for 1 epoch: 13.757873296737671 secs\n",
            "\n",
            "Epoch 193 Batch 0 Loss 0.1071 Accuracy 0.9617\n",
            "Epoch 193 Loss 0.0990 Accuracy 0.9658\n",
            "Time taken for 1 epoch: 13.753590106964111 secs\n",
            "\n",
            "Epoch 194 Batch 0 Loss 0.0757 Accuracy 0.9740\n",
            "Epoch 194 Loss 0.0980 Accuracy 0.9667\n",
            "Time taken for 1 epoch: 13.743731021881104 secs\n",
            "\n",
            "Epoch 195 Batch 0 Loss 0.0809 Accuracy 0.9713\n",
            "Saving checkpoint for epoch 195 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-39\n",
            "Epoch 195 Loss 0.0966 Accuracy 0.9667\n",
            "Time taken for 1 epoch: 14.116436004638672 secs\n",
            "\n",
            "Epoch 196 Batch 0 Loss 0.0657 Accuracy 0.9753\n",
            "Epoch 196 Loss 0.0943 Accuracy 0.9676\n",
            "Time taken for 1 epoch: 13.725484132766724 secs\n",
            "\n",
            "Epoch 197 Batch 0 Loss 0.0753 Accuracy 0.9750\n",
            "Epoch 197 Loss 0.0944 Accuracy 0.9671\n",
            "Time taken for 1 epoch: 13.667749881744385 secs\n",
            "\n",
            "Epoch 198 Batch 0 Loss 0.0748 Accuracy 0.9765\n",
            "Epoch 198 Loss 0.0931 Accuracy 0.9682\n",
            "Time taken for 1 epoch: 13.503977060317993 secs\n",
            "\n",
            "Epoch 199 Batch 0 Loss 0.0795 Accuracy 0.9696\n",
            "Epoch 199 Loss 0.0904 Accuracy 0.9688\n",
            "Time taken for 1 epoch: 13.740713596343994 secs\n",
            "\n",
            "Epoch 200 Batch 0 Loss 0.0813 Accuracy 0.9764\n",
            "Saving checkpoint for epoch 200 at /content/drive/MyDrive/VinBigData/checkpoints/train1/ckpt-40\n",
            "Epoch 200 Loss 0.0931 Accuracy 0.9677\n",
            "Time taken for 1 epoch: 13.934710025787354 secs\n",
            "\n",
            "Epoch 201 Batch 0 Loss 0.0810 Accuracy 0.9713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfcsSWswSdGV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5buvMlnvyrFm"
      },
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "def evaluate(inp_sentence):\n",
        "  start_token = [tokenizer_en.vocab_size]\n",
        "  end_token = [tokenizer_en.vocab_size + 1]\n",
        "  \n",
        "  # inp sentence is eng, hence adding the start and end token\n",
        "  inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  # as the target is vn, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  decoder_input = [tokenizer_vn.vocab_size]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "  \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer_vn.vocab_size+1:\n",
        "      \n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "      \n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU2_yG_vBGza"
      },
      "source": [
        "def translate(sentence, plot=''):\n",
        "  result, attention_weights = evaluate(sentence)\n",
        "  predicted_sentence = tokenizer_vn.decode([i for i in result \n",
        "                                            if i < tokenizer_vn.vocab_size])  \n",
        "\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  \n",
        "  if plot:\n",
        "    plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-NvetisO_zp"
      },
      "source": [
        "print(translate('你好！'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KPmiEdURlkv"
      },
      "source": [
        "print(translate('我爱你')) # anh yêu em"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBqE3S9yPapg"
      },
      "source": [
        "print(translate('听说越南的杂技很有意思，我还没看过呢。'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1JClUjSPynI"
      },
      "source": [
        "print(translate('你来过越南吗？你来越南以后去过什么地方？'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0mV1V2YQA1b"
      },
      "source": [
        "print(translate('我是越南人')) # tôi là người VN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeV9sLi8T2Qy"
      },
      "source": [
        "print(translate('我不是越南人')) # tôi không là người VN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuLevzyESCef"
      },
      "source": [
        "print(translate('他是中国人')) # anh ta là người Trung Quốc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMPoGxdQQKiS"
      },
      "source": [
        "print(translate('越南北部的中国')) # Trung Quốc ở phía bắc Việt Nam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zbGb506PIN9"
      },
      "source": [
        "print(translate('明天我哥哥很忙。'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsxrAlvFG8SZ"
      },
      "source": [
        "print(translate('那是英文杂志。'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7x7m65BOcMX"
      },
      "source": [
        "print(translate('明天下了课我就去办公室找她。'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JQrqaPZO40H"
      },
      "source": [
        "print(translate('真正的失败从来都不是结果的不仅如人意，而是拥有的时候随意挥霍，和未曾用心尝试前的轻言放弃。'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ7zPBmBOQOS"
      },
      "source": [
        "print(translate('你的名字写下来不过几厘米长，却贯穿了我这么长时光。其实你不知道，你一直是我的梦想。'))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}